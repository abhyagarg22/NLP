{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhyagarg22/NLP/blob/main/Basics_of_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)\n",
        "\n",
        "### Objectives:\n",
        "- **To introduce the basic concepts of NLP.**\n",
        "- **To demonstrate real-world applications of NLP.**\n",
        "- **To engage students with interactive examples and exercises.**\n"
      ],
      "metadata": {
        "id": "S7Ffau239vnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is NLP?\n",
        "\n",
        "- **Definition:** Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language.\n",
        "- **Simple Explanation:** Think of NLP as teaching computers to understand and talk in human language.\n",
        "\n",
        "## 2. Need for NLP\n",
        "\n",
        "- **Understanding Human Language:** Computers need to understand human language to provide meaningful responses.\n",
        "- **Automating Repetitive Tasks:** Tasks like sorting emails, summarizing texts, or analyzing sentiments can be automated using NLP.\n",
        "- **Enhancing User Experience:** Virtual assistants, chatbots, and translation services all rely on NLP to improve interactions.\n",
        "\n",
        "## 3. Applications of NLP\n",
        "\n",
        "1. **Machine Translation:** Tools like Google Translate.\n",
        "2. **Sentiment Analysis:** Understanding emotions in texts (e.g., social media posts).\n",
        "3. **Text Summarization:** Creating short summaries of long documents.\n",
        "4. **Speech Recognition:** Voice-activated assistants like Siri or Alexa.\n",
        "5. **Chatbots and Virtual Assistants:** Automated customer service bots.\n",
        "\n",
        "## 4. Basic Steps in NLP\n",
        "\n",
        "1. **Text Preprocessing:** Cleaning and preparing text data.\n",
        "2. **Tokenization:** Splitting text into individual words or phrases.\n",
        "3. **Removing Stop Words:** Filtering out common words that add little meaning.\n",
        "4. **Stemming and Lemmatization:** Reducing words to their root forms.\n",
        "5. **Vectorization:** Converting text into numerical vectors.\n",
        "6. **Model Building:** Creating machine learning models to analyze text.\n",
        "7. **Evaluation:** Assessing the performance of the models.\n"
      ],
      "metadata": {
        "id": "vQXHh3P19vne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstration Outline:\n",
        "\n",
        "## 1. Text Preprocessing:"
      ],
      "metadata": {
        "id": "G1lIfNVy9vnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "sample_text = \"Hello World! This is a sample text for NLP preprocessing.\"\n",
        "processed_text = preprocess_text(sample_text)\n",
        "print(processed_text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T04:35:21.578087Z",
          "iopub.execute_input": "2024-07-08T04:35:21.578485Z",
          "iopub.status.idle": "2024-07-08T04:35:21.615312Z",
          "shell.execute_reply.started": "2024-07-08T04:35:21.578452Z",
          "shell.execute_reply": "2024-07-08T04:35:21.614069Z"
        },
        "trusted": true,
        "id": "xxmhhfOF9vnf",
        "outputId": "56c95fde-887c-4ac8-c730-b679f8a98a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "hello world this is a sample text for nlp preprocessing\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenisation"
      ],
      "metadata": {
        "id": "TPQ6mQww9vng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(processed_text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T04:35:29.171797Z",
          "iopub.execute_input": "2024-07-08T04:35:29.172807Z",
          "iopub.status.idle": "2024-07-08T04:35:30.759593Z",
          "shell.execute_reply.started": "2024-07-08T04:35:29.17277Z",
          "shell.execute_reply": "2024-07-08T04:35:30.758456Z"
        },
        "trusted": true,
        "id": "GoDUh3nM9vng",
        "outputId": "4959857d-8acc-45d9-bfaa-56c20ac2b7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n['hello', 'world', 'this', 'is', 'a', 'sample', 'text', 'for', 'nlp', 'preprocessing']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Removing Stop Words:"
      ],
      "metadata": {
        "id": "DzSoQiB_9vng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T04:37:10.050085Z",
          "iopub.execute_input": "2024-07-08T04:37:10.050487Z",
          "iopub.status.idle": "2024-07-08T04:37:10.065778Z",
          "shell.execute_reply.started": "2024-07-08T04:37:10.050456Z",
          "shell.execute_reply": "2024-07-08T04:37:10.064575Z"
        },
        "trusted": true,
        "id": "96cdqjhC9vnh",
        "outputId": "fb242823-13ac-48cc-bce3-5685ce321ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n['hello', 'world', 'sample', 'text', 'nlp', 'preprocessing']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:03:27.674215Z",
          "iopub.execute_input": "2024-07-08T05:03:27.674643Z",
          "iopub.status.idle": "2024-07-08T05:03:27.680727Z",
          "shell.execute_reply.started": "2024-07-08T05:03:27.674615Z",
          "shell.execute_reply": "2024-07-08T05:03:27.679648Z"
        },
        "trusted": true,
        "id": "ScoHOJ0I9vnh",
        "outputId": "bc420e61-18d2-4759-99b8-cb8fddf2f6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{\"don't\", 're', 'again', 'be', 'yours', 'when', 'most', 'hadn', 'can', 'y', 'do', 'yourselves', 'such', 'have', \"you'll\", 'yourself', 'some', 'which', 'in', 'they', 'itself', 'no', 'whom', 'down', 'nor', 'and', \"couldn't\", 'were', 'am', 'shouldn', 'doing', 's', \"shouldn't\", 'on', \"mustn't\", 'his', 'a', 'under', 'weren', 'through', 'me', 'should', 'above', 'we', 'if', 'out', 'there', 'himself', 'its', \"it's\", 'other', 'wouldn', 'at', \"hadn't\", \"shan't\", 'over', 'is', 'during', 'she', \"she's\", 'after', 'm', 'themselves', 'while', 'your', \"you're\", 'isn', 'all', 'then', 'd', 'further', 'ourselves', 'here', 'was', 'same', 'i', 'them', 'to', 'he', 'or', 'too', \"doesn't\", 'own', 'but', 'her', 'those', 'where', 'into', \"wasn't\", 'for', \"you've\", 'will', \"didn't\", 'my', 'of', 'who', 'hers', 'theirs', 'only', \"that'll\", 'doesn', 'what', 'from', 'once', 'until', 'having', \"hasn't\", 'haven', 'mustn', 'ain', \"aren't\", 'aren', 'few', 'between', 'up', 'as', \"won't\", 'that', 'because', 'off', 'both', 'has', 'are', 'very', 'been', 'our', 'about', 'didn', \"should've\", \"wouldn't\", 'their', 'with', 'how', 'than', 'you', 'had', 'more', \"isn't\", 'won', 'mightn', 'just', 'him', 'did', \"mightn't\", 'shan', \"weren't\", 'these', 'ma', 'wasn', \"haven't\", 'ours', 'against', 'couldn', 'hasn', 'herself', 't', 'not', 'll', 'needn', 've', 'each', 'now', 'an', 'the', 'o', 'being', 'myself', 'by', 'before', \"you'd\", 'this', 'so', \"needn't\", 'it', 'below', 'any', 'does', 'don', 'why'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Stemming"
      ],
      "metadata": {
        "id": "bjKWWxOD9vnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_tokens)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T04:58:15.09272Z",
          "iopub.execute_input": "2024-07-08T04:58:15.093665Z",
          "iopub.status.idle": "2024-07-08T04:58:15.102651Z",
          "shell.execute_reply.started": "2024-07-08T04:58:15.09362Z",
          "shell.execute_reply": "2024-07-08T04:58:15.101284Z"
        },
        "trusted": true,
        "id": "ylq9hJOE9vnh",
        "outputId": "6e016254-3031-40e4-ea15-ff4ab85100fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['hello', 'world', 'sampl', 'text', 'nlp', 'preprocess']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Lemmatization:"
      ],
      "metadata": {
        "id": "ktpqJFol9vnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "# Download the necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Required for wordnet\n",
        "nltk.download('punkt')  # Ensure punkt is downloaded for tokenization\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example text\n",
        "text = \"The boy was going for a trip where he could say that he hiked, danced, sung, swam, surfed and cooked.\"\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example list of filtered tokens\n",
        "filtered_tokens = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
        "\n",
        "# Check if the required NLTK resources are available\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet.zip')\n",
        "    nltk.data.find('corpora/omw-1.4.zip')\n",
        "    # Lemmatize each token\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    print(\"Lemmatized tokens using NLTK:\", lemmatized_tokens)\n",
        "except LookupError:\n",
        "    print(\"WordNet resource not found. Please ensure it is downloaded properly.\")\n",
        "\n",
        "# Additionally, lemmatize using spaCy\n",
        "doc = nlp(text)\n",
        "spacy_lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(\"Lemmatized tokens using spaCy:\", spacy_lemmatized_tokens)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:10:40.987013Z",
          "iopub.execute_input": "2024-07-08T05:10:40.987517Z",
          "iopub.status.idle": "2024-07-08T05:10:41.858106Z",
          "shell.execute_reply.started": "2024-07-08T05:10:40.987484Z",
          "shell.execute_reply": "2024-07-08T05:10:41.856883Z"
        },
        "trusted": true,
        "id": "uzMjPpfs9vnh",
        "outputId": "ab75fc02-ecd2-448b-cb36-384708779f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nWordNet resource not found. Please ensure it is downloaded properly.\nLemmatized tokens using spaCy: ['the', 'boy', 'be', 'go', 'for', 'a', 'trip', 'where', 'he', 'could', 'say', 'that', 'he', 'hike', ',', 'danced', ',', 'sung', ',', 'swam', ',', 'surfed', 'and', 'cook', '.']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:10:46.255961Z",
          "iopub.execute_input": "2024-07-08T05:10:46.256329Z",
          "iopub.status.idle": "2024-07-08T05:10:46.395808Z",
          "shell.execute_reply.started": "2024-07-08T05:10:46.256303Z",
          "shell.execute_reply": "2024-07-08T05:10:46.394156Z"
        },
        "trusted": true,
        "id": "N4rB9Y599vnh",
        "outputId": "f3402c09-42a3-4464-a96d-d0554b3a6688"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:80\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/.zip/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124momw-1.4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 6\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokens)\n",
            "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124momw-1.4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m----> 6\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_tokens)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/wordnet.py:40\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, pos\u001b[38;5;241m=\u001b[39mNOUN):\n\u001b[0;32m---> 40\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:116\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/corpus/util.py:78\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir, zip_name))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/data.py:653\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    652\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
          ],
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Vectorization"
      ],
      "metadata": {
        "id": "QZd9Wri39vni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "texts = [\"I love programming.\", \"Python is awesome.\", \"I hate bugs.\", \"Debugging is fun.\"]\n",
        "labels = [1, 1, 0, 1]  # 1 for positive, 0 for negative\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "vectorized_texts = tfidf_vectorizer.fit_transform(texts)\n",
        "print(vectorized_texts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:16:24.373185Z",
          "iopub.execute_input": "2024-07-08T05:16:24.373634Z",
          "iopub.status.idle": "2024-07-08T05:16:24.399835Z",
          "shell.execute_reply.started": "2024-07-08T05:16:24.373604Z",
          "shell.execute_reply": "2024-07-08T05:16:24.398682Z"
        },
        "trusted": true,
        "id": "1CkIuJBn9vni",
        "outputId": "5fd05ffb-2035-444b-b0aa-97d31a5971b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "  (0, 7)\t0.7071067811865476\n  (0, 6)\t0.7071067811865476\n  (1, 0)\t0.6176143709756019\n  (1, 5)\t0.48693426407352264\n  (1, 8)\t0.6176143709756019\n  (2, 1)\t0.7071067811865476\n  (2, 4)\t0.7071067811865476\n  (3, 3)\t0.6176143709756019\n  (3, 2)\t0.6176143709756019\n  (3, 5)\t0.48693426407352264\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Simple Model Building:"
      ],
      "metadata": {
        "id": "FDVKmL369vni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(vectorized_texts, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T05:16:30.142257Z",
          "iopub.execute_input": "2024-07-08T05:16:30.142643Z",
          "iopub.status.idle": "2024-07-08T05:16:30.159328Z",
          "shell.execute_reply.started": "2024-07-08T05:16:30.142615Z",
          "shell.execute_reply": "2024-07-08T05:16:30.158025Z"
        },
        "trusted": true,
        "id": "-pdkRQmY9vni",
        "outputId": "780f7852-79b2-4a50-9c10-9876efdcedf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 100.00%\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}